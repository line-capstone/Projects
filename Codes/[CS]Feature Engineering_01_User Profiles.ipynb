{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering - 1) User Profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: 분석을 위한 사전 환경 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A-1) 모듈/패키지 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket 관련 사전 설정\n",
    "\n",
    "OUTPUT_BUCKET_FOLDER = \"gs://cap-18/output/\"\n",
    "DATA_BUCKET_FOLDER = \"gs://cap-18/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 모듈 로드\n",
    "\n",
    "from IPython.display import display\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrameWriter\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "random.seed(42)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = True\n",
    "truncate_day_from_timestamp_udf = F.udf(lambda ts: int(ts / 1000 / 60 / 60 / 24), IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A-2) 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# country를 추출하는 함수 생성\n",
    "extract_country_udf = F.udf(lambda geo: geo.strip()[:2] if geo != None else '', StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents_meta 스키마(document_id_doc, source_id, publisher_id, publish_time) 생성\n",
    "documents_meta_schema = StructType(\n",
    "                    [StructField(\"document_id_doc\", IntegerType(), True),\n",
    "                    StructField(\"source_id\", IntegerType(), True),                    \n",
    "                    StructField(\"publisher_id\", IntegerType(), True),\n",
    "                    StructField(\"publish_time\", TimestampType(), True)]\n",
    "                    )\n",
    "\n",
    "# documents_meta_df 테이블 생성\n",
    "documents_meta_df = spark.read.schema(documents_meta_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"documents_meta.csv\") \\\n",
    "                .withColumn('dummyDocumentsMeta', F.lit(1)).alias('documents_meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+------------+-------------------+------------------+\n",
      "|document_id_doc|source_id|publisher_id|       publish_time|dummyDocumentsMeta|\n",
      "+---------------+---------+------------+-------------------+------------------+\n",
      "|        1595802|        1|         603|2016-06-05 00:00:00|                 1|\n",
      "|        1524246|        1|         603|2016-05-26 11:00:00|                 1|\n",
      "|        1617787|        1|         603|2016-05-27 00:00:00|                 1|\n",
      "|        1615583|        1|         603|2016-06-07 00:00:00|                 1|\n",
      "|        1615460|        1|         603|2016-06-20 00:00:00|                 1|\n",
      "|        1615354|        1|         603|2016-06-10 00:00:00|                 1|\n",
      "|        1614611|        1|         603|2016-06-05 13:00:00|                 1|\n",
      "|        1614235|        1|         603|2016-06-09 00:00:00|                 1|\n",
      "|        1614225|        1|         603|2016-06-09 00:00:00|                 1|\n",
      "|        1488264|        1|         603|2016-05-23 13:00:00|                 1|\n",
      "+---------------+---------+------------+-------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documents_meta_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents_categories 스키마 생성\n",
    "documents_categories_schema = StructType(\n",
    "                    [StructField(\"document_id_cat\", IntegerType(), True),\n",
    "                    StructField(\"category_id\", IntegerType(), True),                    \n",
    "                    StructField(\"confidence_level_cat\", FloatType(), True)]\n",
    "                    )\n",
    "\n",
    "documents_categories_df = spark.read.schema(documents_categories_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"documents_categories.csv\") \\\n",
    "                .alias('documents_categories')\n",
    "\n",
    "# documents_categories_grouped_df 생성\n",
    "documents_categories_grouped_df = documents_categories_df.groupBy('document_id_cat') \\ # documment_id_cat별로 묶은 후,\n",
    "                                            .agg(F.collect_list('category_id').alias('category_id_list'), # category_id_list 컬럼: category_id를 list로 묶어 표시\n",
    "                                                 F.collect_list('confidence_level_cat').alias('cat_confidence_level_list')) \\ # cat_confidence_level_list 컬럼: category의 conf. level을 list로 묶어 표시  \n",
    "                                            .withColumn('dummyDocumentsCategory', F.lit(1)) \\ # documents_category인지 여부를 dummy로 표시\n",
    "                                            .alias('documents_categories_grouped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+-------------------------+----------------------+\n",
      "|document_id_cat|category_id_list|cat_confidence_level_list|dummyDocumentsCategory|\n",
      "+---------------+----------------+-------------------------+----------------------+\n",
      "|            148|    [1403, 1702]|             [0.92, 0.07]|                     1|\n",
      "|            463|    [1513, 1808]|     [0.8932095, 0.067...|                     1|\n",
      "+---------------+----------------+-------------------------+----------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# documents_categories_grouped_df 구조 확인\n",
    "documents_categories_grouped_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents_topics 스키마 생성\n",
    "documents_topics_schema = StructType(\n",
    "                    [StructField(\"document_id_top\", IntegerType(), True),\n",
    "                    StructField(\"topic_id\", IntegerType(), True),                    \n",
    "                    StructField(\"confidence_level_top\", FloatType(), True)]\n",
    "                    )\n",
    "\n",
    "documents_topics_df = spark.read.schema(documents_topics_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"documents_topics.csv\")  \\\n",
    "                .alias('documents_topics')\n",
    "\n",
    "# documents_topics_grouped_ef 생성\n",
    "documents_topics_grouped_df = documents_topics_df.groupBy('document_id_top') \\ # document_id_top별로 묶은 후,\n",
    "                                            .agg(F.collect_list('topic_id').alias('topic_id_list'), # topic_id_list 컬럼: topic_id를 list로 묶어 표시\n",
    "                                                 F.collect_list('confidence_level_top').alias('top_confidence_level_list')) \\ # top_confidence_level_list 컬럼: topic의 conf. level을 list로 묶어 표시\n",
    "                                            .withColumn('dummyDocumentsTopics', F.lit(1)) \\ # documents_topics인지 여부를 dummy로 표시\n",
    "                                            .alias('documents_topics_grouped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents_df 생성: documents_meta-documents_categories_grouped_df-documents_topics_grouped_df join\n",
    "documents_df = documents_meta_df.join(documents_categories_grouped_df, on=F.col(\"document_id_doc\") == F.col(\"documents_categories_grouped.document_id_cat\"), how='left') \\\n",
    "                         .join(documents_topics_grouped_df, on=F.col(\"document_id_doc\") == F.col(\"documents_topics_grouped.document_id_top\"), how='left') \\\n",
    "                         .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2999334"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+------------+-------------------+------------------+---------------+----------------+-------------------------+----------------------+---------------+--------------------+-------------------------+--------------------+---------------+--------------------+-------------------------+----------------------+\n",
      "|document_id_doc|source_id|publisher_id|       publish_time|dummyDocumentsMeta|document_id_cat|category_id_list|cat_confidence_level_list|dummyDocumentsCategory|document_id_top|       topic_id_list|top_confidence_level_list|dummyDocumentsTopics|document_id_ent|      entity_id_list|ent_confidence_level_list|dummyDocumentsEntities|\n",
      "+---------------+---------+------------+-------------------+------------------+---------------+----------------+-------------------------+----------------------+---------------+--------------------+-------------------------+--------------------+---------------+--------------------+-------------------------+----------------------+\n",
      "|            148|     1787|         118|2008-07-01 00:00:00|                 1|            148|    [1403, 1702]|             [0.92, 0.07]|                     1|            148|[153, 140, 8, 172...|     [0.07523697, 0.07...|                   1|            148|[e1c74838563ef5d2...|     [0.6320258, 0.404...|                     1|\n",
      "|            463|      166|         642|2008-12-09 19:00:00|                 1|            463|    [1513, 1808]|     [0.8932095, 0.067...|                     1|            463|[181, 292, 24, 25...|     [0.11870128, 0.05...|                   1|            463|[aaa0246895d43735...|              [0.6939791]|                     1|\n",
      "+---------------+---------+------------+-------------------+------------------+---------------+----------------+-------------------------+----------------------+---------------+--------------------+-------------------------+--------------------+---------------+--------------------+-------------------------+----------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documents_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- document_id_doc: integer (nullable = true)\n",
      " |-- source_id: integer (nullable = true)\n",
      " |-- publisher_id: integer (nullable = true)\n",
      " |-- publish_time: timestamp (nullable = true)\n",
      " |-- dummyDocumentsMeta: integer (nullable = false)\n",
      " |-- document_id_cat: integer (nullable = true)\n",
      " |-- category_id_list: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- cat_confidence_level_list: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- dummyDocumentsCategory: integer (nullable = true)\n",
      " |-- document_id_top: integer (nullable = true)\n",
      " |-- topic_id_list: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- top_confidence_level_list: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- dummyDocumentsTopics: integer (nullable = true)\n",
      " |-- document_id_ent: integer (nullable = true)\n",
      " |-- entity_id_list: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- ent_confidence_level_list: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- dummyDocumentsEntities: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documents_df.printSchema() # documents_df 구조 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    validation_set_df = spark.read.parquet(OUTPUT_BUCKET_FOLDER+\"validation_set.parquet\") \\\n",
    "                    .alias('validation_set')\n",
    "    \n",
    "    validation_set_df.select('uuid_event').distinct().createOrReplaceTempView('users_to_profile')    \n",
    "    validation_set_df.select('uuid_event','document_id_promo').distinct().createOrReplaceTempView('validation_users_docs_to_ignore')\n",
    "    \n",
    "else:\n",
    "    events_schema = StructType(\n",
    "                    [StructField(\"display_id\", IntegerType(), True),\n",
    "                    StructField(\"uuid_event\", StringType(), True),                    \n",
    "                    StructField(\"document_id_event\", IntegerType(), True),\n",
    "                    StructField(\"timestamp_event\", IntegerType(), True),\n",
    "                    StructField(\"platform_event\", IntegerType(), True),\n",
    "                    StructField(\"geo_location_event\", StringType(), True)]\n",
    "                    )\n",
    "\n",
    "    events_df = spark.read.schema(events_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                    .csv(DATA_BUCKET_FOLDER+\"events.csv\") \\\n",
    "                    .withColumn('dummyEvents', F.lit(1)) \\\n",
    "                    .withColumn('day_event', truncate_day_from_timestamp_udf('timestamp_event')) \\\n",
    "                    .withColumn('event_country', extract_country_udf('geo_location_event')) \\\n",
    "                    .alias('events')\n",
    "\n",
    "    events_df.createOrReplaceTempView('events')\n",
    "\n",
    "\n",
    "    promoted_content_schema = StructType(\n",
    "                        [StructField(\"ad_id\", IntegerType(), True),\n",
    "                        StructField(\"document_id_promo\", IntegerType(), True),                    \n",
    "                        StructField(\"campaign_id\", IntegerType(), True),\n",
    "                        StructField(\"advertiser_id\", IntegerType(), True)]\n",
    "                        )\n",
    "\n",
    "    promoted_content_df = spark.read.schema(promoted_content_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                    .csv(DATA_BUCKET_FOLDER+\"promoted_content.csv\") \\\n",
    "                    .withColumn('dummyPromotedContent', F.lit(1)).alias('promoted_content')\n",
    "\n",
    "    clicks_test_schema = StructType(\n",
    "                        [StructField(\"display_id\", IntegerType(), True),\n",
    "                        StructField(\"ad_id\", IntegerType(), True)]\n",
    "                        )\n",
    "\n",
    "    clicks_test_df = spark.read.schema(clicks_test_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                    .csv(DATA_BUCKET_FOLDER+\"clicks_test.csv\") \\\n",
    "                    .withColumn('dummyClicksTest', F.lit(1)).alias('clicks_test')\n",
    "    \n",
    "    test_set_df = clicks_test_df.join(promoted_content_df, on='ad_id', how='left') \\\n",
    "                                .join(events_df, on='display_id', how='left')\n",
    "        \n",
    "    test_set_df.select('uuid_event').distinct().createOrReplaceTempView('users_to_profile')\n",
    "    test_set_df.select('uuid_event','document_id_promo', 'timestamp_event').distinct().createOrReplaceTempView('test_users_docs_timestamp_to_ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page_views 구조 생성\n",
    "page_views_schema = StructType(\n",
    "                    [StructField(\"uuid_pv\", StringType(), True),\n",
    "                    StructField(\"document_id_pv\", IntegerType(), True),\n",
    "                    StructField(\"timestamp_pv\", IntegerType(), True),\n",
    "                    StructField(\"platform_pv\", IntegerType(), True),\n",
    "                    StructField(\"geo_location_pv\", StringType(), True),\n",
    "                    StructField(\"traffic_source_pv\", IntegerType(), True)]\n",
    "                    )\n",
    "\n",
    "page_views_df = spark.read.schema(page_views_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"page_views.csv\") \\\n",
    "                .alias('page_views')        \n",
    "            \n",
    "page_views_df.createOrReplaceTempView('page_views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_filter = ''\n",
    "if evaluation:\n",
    "    additional_filter = '''\n",
    "                             AND NOT EXISTS (SELECT uuid_event FROM validation_users_docs_to_ignore \n",
    "                                                      WHERE uuid_event = p.uuid_pv\n",
    "                                                     AND document_id_promo = p.document_id_pv)\n",
    "                        '''\n",
    "else:\n",
    "    additional_filter = '''\n",
    "                             AND NOT EXISTS (SELECT uuid_event FROM test_users_docs_timestamp_to_ignore \n",
    "                                                      WHERE uuid_event = p.uuid_pv\n",
    "                                                     AND document_id_promo = p.document_id_pv\n",
    "                                                     AND p.timestamp_pv >= timestamp_event)\n",
    "                        '''\n",
    "\n",
    "page_views_train_df = spark.sql('''SELECT * FROM page_views p \n",
    "                                    WHERE EXISTS (SELECT uuid_event FROM users_to_profile\n",
    "                                                 WHERE uuid_event = p.uuid_pv)                                     \n",
    "                                '''+ additional_filter\n",
    "                               ).alias('views') \\\n",
    "                         .join(documents_df, on=F.col(\"document_id_pv\") == F.col(\"document_id_doc\"), how='left') \\\n",
    "                         .filter('dummyDocumentsEntities is not null OR dummyDocumentsTopics is not null OR dummyDocumentsCategory is not null')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing document frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2999334"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_total = documents_meta_df.count()\n",
    "documents_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_docs_counts = documents_categories_df.groupBy('category_id').count().rdd.collectAsMap()\n",
    "len(categories_docs_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filenames_suffix = ''\n",
    "if evaluation:\n",
    "    df_filenames_suffix = '_eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('categories_docs_counts'+df_filenames_suffix+'.pickle', 'wb') as output:\n",
    "    pickle.dump(categories_docs_counts, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_docs_counts = documents_topics_df.groupBy('topic_id').count().rdd.collectAsMap()\n",
    "len(topics_docs_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('topics_docs_counts'+df_filenames_suffix+'.pickle', 'wb') as output:\n",
    "    pickle.dump(topics_docs_counts, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1326009"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_docs_counts = documents_entities_df.groupBy('entity_id').count().rdd.collectAsMap()\n",
    "len(entities_docs_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing User Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_null_to_minus_one_udf = F.udf(lambda x: x if x != None else -1, IntegerType())\n",
    "int_list_null_to_empty_list_udf = F.udf(lambda x: x if x != None else [], ArrayType(IntegerType()))\n",
    "float_list_null_to_empty_list_udf = F.udf(lambda x: x if x != None else [], ArrayType(FloatType()))\n",
    "str_list_null_to_empty_list_udf = F.udf(lambda x: x if x != None else [], ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형 변환\n",
    "\n",
    "page_views_by_user_df = page_views_train_df.select(\n",
    "                           'uuid_pv', \n",
    "                           'document_id_pv', \n",
    "                           int_null_to_minus_one_udf('timestamp_pv').alias('timestamp_pv'), \n",
    "                           int_list_null_to_empty_list_udf('category_id_list').alias('category_id_list'), \n",
    "                           float_list_null_to_empty_list_udf('cat_confidence_level_list').alias('cat_confidence_level_list'), \n",
    "                           int_list_null_to_empty_list_udf('topic_id_list').alias('topic_id_list'), \n",
    "                           float_list_null_to_empty_list_udf('top_confidence_level_list').alias('top_confidence_level_list')) \\\n",
    "                    .groupBy('uuid_pv') \\\n",
    "                    .agg(F.collect_list('document_id_pv').alias('document_id_pv_list'),\n",
    "                         F.collect_list('timestamp_pv').alias('timestamp_pv_list'),\n",
    "                         F.collect_list('category_id_list').alias('category_id_lists'),\n",
    "                         F.collect_list('cat_confidence_level_list').alias('cat_confidence_level_lists'),\n",
    "                         F.collect_list('topic_id_list').alias('topic_id_lists'),\n",
    "                         F.collect_list('top_confidence_level_list').alias('top_confidence_level_lists')\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# get_user_aspects 함수 생성:\n",
    "def get_user_aspects(docs_aspects, aspect_docs_counts):\n",
    "    docs_aspects_merged_lists = defaultdict(list)\n",
    "    \n",
    "    for doc_aspects in docs_aspects: \n",
    "        for key in doc_aspects.keys(): \n",
    "            docs_aspects_merged_lists[key].append(doc_aspects[key])\n",
    "        \n",
    "    docs_aspects_stats = {}\n",
    "    for key in docs_aspects_merged_lists.keys():\n",
    "        aspect_list = docs_aspects_merged_lists[key]\n",
    "        tf = len(aspect_list)\n",
    "        idf = math.log(documents_total / float(aspect_docs_counts[key]))\n",
    "        \n",
    "        confid_mean = sum(aspect_list) / float(len(aspect_list))\n",
    "        docs_aspects_stats[key] = [tf*idf, confid_mean]\n",
    "\n",
    "        \n",
    "    return docs_aspects_stats\n",
    "\n",
    "# generate_user_profile 함수 생성:\n",
    "def generate_user_profile(docs_aspects_list, docs_aspects_confidence_list, aspect_docs_counts):    \n",
    "    docs_aspects = []\n",
    "    for doc_aspects_list, doc_aspects_confidence_list in zip(docs_aspects_list, docs_aspects_confidence_list):\n",
    "        doc_aspects = dict(zip(doc_aspects_list, doc_aspects_confidence_list))\n",
    "        docs_aspects.append(doc_aspects)\n",
    "        \n",
    "    user_aspects = get_user_aspects(docs_aspects, aspect_docs_counts)\n",
    "    return user_aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_list_len_udf = F.udf(lambda docs_list: len(docs_list), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_categories_user_profile_map_udf = F.udf(lambda docs_aspects_list, \n",
    "                                                 docs_aspects_confidence_list: \\\n",
    "                                                      generate_user_profile(docs_aspects_list, \n",
    "                                                                            docs_aspects_confidence_list, \n",
    "                                                                            categories_docs_counts), \n",
    "                                          MapType(IntegerType(), \n",
    "                                                  ArrayType(FloatType()),\n",
    "                                                  False))\n",
    "\n",
    "\n",
    "generate_topics_user_profile_map_udf = F.udf(lambda docs_aspects_list, \n",
    "                                                 docs_aspects_confidence_list: \\\n",
    "                                                      generate_user_profile(docs_aspects_list, \n",
    "                                                                            docs_aspects_confidence_list, \n",
    "                                                                            topics_docs_counts), \n",
    "                                          MapType(IntegerType(), \n",
    "                                                  ArrayType(FloatType()),\n",
    "                                                  False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_profile_df = page_views_by_user_df \\\n",
    "                                 .withColumn('views', get_list_len_udf('document_id_pv_list')) \\\n",
    "                                 .withColumn('categories', \n",
    "                                             generate_categories_user_profile_map_udf('category_id_lists', \n",
    "                                                                   'cat_confidence_level_lists')) \\\n",
    "                                 .withColumn('topics', \n",
    "                                             generate_topics_user_profile_map_udf('topic_id_lists', \n",
    "                                                                               'top_confidence_level_lists')) \\\n",
    "                                 .select(F.col('uuid_pv').alias('uuid'),\n",
    "                                         F.col('document_id_pv_list').alias('doc_ids'),\n",
    "                                         'views',\n",
    "                                         'categories', 'topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    table_name = 'user_profiles_eval'\n",
    "else:\n",
    "    table_name = 'user_profiles'\n",
    "\n",
    "users_profile_df.write.parquet(OUTPUT_BUCKET_FOLDER+table_name, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Elapsed min: ', 0.3151605778270297)\n"
     ]
    }
   ],
   "source": [
    "finish_time = time.time()\n",
    "print(\"Elapsed min: \", (finish_time-start_time)/60/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|          uuid|             doc_ids|views|          categories|              topics|            entities|\n",
      "+--------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|100013af048bbf|[2447063, 2444477...|   46|Map(1205 -> Wrapp...|Map(5 -> WrappedA...|Map(14a7d4c4ebcc6...|\n",
      "|100163b35102c4|[2516821, 2356657...|   12|Map(1907 -> Wrapp...|Map(174 -> Wrappe...|Map(6904a5638b5cf...|\n",
      "|1003370a1c2d0f|[1521640, 2053639...|    4|Map(1808 -> Wrapp...|Map(69 -> Wrapped...|Map(531cadf46e145...|\n",
      "|100659017f177b|            [429642]|    1|Map(1510 -> Wrapp...|Map(296 -> Wrappe...|               Map()|\n",
      "|100aa12f880396|[1792136, 2504276...|    3|Map(1408 -> Wrapp...|Map(265 -> Wrappe...|Map(b366917165b76...|\n",
      "|101324634e39b0|  [2672785, 2690250]|    2|Map(1210 -> Wrapp...|Map(20 -> Wrapped...|Map(14a7d4c4ebcc6...|\n",
      "|101487b48a7780|[2713662, 428673,...|   44|Map(1406 -> Wrapp...|Map(138 -> Wrappe...|Map(5096ae94f0e6d...|\n",
      "|1014e25bc11b1b|[1814000, 1399046...|    7|Map(1406 -> Wrapp...|Map(249 -> Wrappe...|Map(b97df21a91594...|\n",
      "|101bd04e3e07d7|           [2111911]|    1|Map(1100 -> Wrapp...|Map(232 -> Wrappe...|Map(d78596b7a5bdc...|\n",
      "|101dae0b4172f6|[2418906, 2398523...|    4|Map(1708 -> Wrapp...|Map(10 -> Wrapped...|Map(6de5d563d57c4...|\n",
      "+--------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# users_profile_df의 구조 확인\n",
    "users_profile_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4961756"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_profile_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_profile_df.select('uuid').distinct().show()\n",
    "\n",
    "# 유저별로 누적적(?)으로 page_view 정보가 저장되는가?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
