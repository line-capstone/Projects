{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering - 0) Validation Set Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of Dec 02, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Google Storage Bucket 연결\n",
    "\n",
    "OUTPUT_BUCKET_FOLDER = \"gs://cap-18/output/\" # 작업 결과물을 저장하는 path\n",
    "DATA_BUCKET_FOLDER = \"gs://cap-18/data/\" # 작업을 위한 데이터를 저장해놓은 path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import * # 데이터 프레임 생성 + 형태(type)를 위한 모듈 로드\n",
    "import pyspark.sql.functions as F # 내장 함수 활용을 위한 모듈 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 시간 정보를 변환하기 위한 사용자 정의 함수 생성\n",
    "\n",
    "truncate_day_from_timestamp_udf = F.udf(lambda ts: int(ts / 1000 / 60 / 60 / 24), IntegerType()) # 정수 형태로 output을 반환\n",
    "\n",
    "# cf.) ts / 1000 / 60 / 60 / 24: 밀리세컨드 단위를 1000, 60, 60, 24으로 나누면 일(day) 단위로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-17-789313a30f87>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-789313a30f87>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    events_df = spark.read.schema(events_schema).options(header='true', inferschema='false', nullValue='\\\\N')                 .csv(DATA_BUCKET_FOLDER + \"events.csv\")                 .withColumn('day_event', truncate_day_from_timestamp_udf('timestamp_event')) \\\u001b[0m\n\u001b[0m                                                                                                                                                                                                                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "## events_df 테이블 생성\n",
    "\n",
    "# events_df 테이블의 구조(스키마) 사전 설정\n",
    "events_schema = StructType(\n",
    "                    [StructField(\"display_id\", IntegerType(), True),\n",
    "                    StructField(\"uuid_event\", StringType(), True),                    \n",
    "                    StructField(\"document_id_event\", IntegerType(), True),\n",
    "                    StructField(\"timestamp_event\", IntegerType(), True),\n",
    "                    StructField(\"platform_event\", IntegerType(), True),\n",
    "                    StructField(\"geo_location_event\", StringType(), True)]\n",
    "                    )\n",
    "\n",
    "# events_df 테이블의 구조에 맞춰 events.csv 파일 로드\n",
    "events_df = spark.read.schema(events_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER + \"events.csv\") \\\n",
    "                .withColumn('day_event', truncate_day_from_timestamp_udf('timestamp_event')) \\\n",
    "                .alias('events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a08231ac3d55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# promoted_content_df 테이블의 구조에 맞춰 promoted_content.csv 파일 로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mpromoted_content_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpromoted_content_schema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'false'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnullValue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\\\N'\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_BUCKET_FOLDER\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"promoted_content.csv\"\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'promoted_content'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "## promoted_content_df 테이블 생성\n",
    "\n",
    "# promoted_content_df 테이블의 구조 사전 설정\n",
    "promoted_content_schema = StructType(\n",
    "                    [StructField(\"ad_id\", IntegerType(), True),\n",
    "                    StructField(\"document_id_promo\", IntegerType(), True),                    \n",
    "                    StructField(\"campaign_id\", IntegerType(), True),\n",
    "                    StructField(\"advertiser_id\", IntegerType(), True)]\n",
    "                    )\n",
    "\n",
    "# promoted_content_df 테이블의 구조에 맞춰 promoted_content.csv 파일 로드\n",
    "promoted_content_df = spark.read.schema(promoted_content_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"promoted_content.csv\") \\\n",
    "                .alias('promoted_content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## clicks_train_df 테이블 생성\n",
    "\n",
    "# clicks_train_df 테이블의 구조 사전 설정\n",
    "clicks_train_schema = StructType(\n",
    "                    [StructField(\"display_id\", IntegerType(), True),\n",
    "                    StructField(\"ad_id\", IntegerType(), True),                    \n",
    "                    StructField(\"clicked\", IntegerType(), True)]\n",
    "                    )\n",
    "\n",
    "# clicks_train_df 테이블의 구조에 맞춰 clicks_train.csv 파일 로드\n",
    "clicks_train_df = spark.read.schema(clicks_train_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"clicks_train.csv\") \\\n",
    "                .alias('clicks_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## clicks_train_joined_df 테이블 생성\n",
    "\n",
    "clicks_train_joined_df = clicks_train_df \\\n",
    "                         .join(promoted_content_df, on='ad_id', how='left') \\\n",
    "                         .join(events_df, on='display_id', how='left')                         \n",
    "clicks_train_joined_df.createOrReplaceTempView('clicks_train_joined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## -- clicks_train_joined에서 일자별, 일정 비율로 데이터 샘플링하여 validation_set 생성\n",
    "\n",
    "validation_display_ids_df = clicks_train_joined_df.select('display_id','day_event').distinct() \\\n",
    "                                .sampleBy(\"day_event\", fractions={0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2, \\\n",
    "                                                                5: 0.2, 6: 0.2, 7: 0.2, 8: 0.2, 9: 0.2, 10: 0.2, \\\n",
    "                                                               11: 1.0, 12: 1.0}, seed=0)   \n",
    "validation_display_ids_df.createOrReplaceTempView(\"validation_display_ids\")                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_set_df = spark.sql('''SELECT display_id, ad_id, uuid_event, day_event, timestamp_event,\n",
    "                                        document_id_promo, platform_event, geo_location_event FROM clicks_train_joined t \n",
    "             WHERE EXISTS (SELECT display_id FROM validation_display_ids \n",
    "                           WHERE display_id = t.display_id)''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I/O를 최소한으로 유지하기 위해 컬럼 기반 저장 포맷인 파케이(Parquet)로 저장하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_set_gcs_output = \"validation_set.parquet\"\n",
    "validation_set_df.write.parquet(OUTPUT_BUCKET_FOLDER+validation_set_gcs_output, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "혹시 모를 일(?)을 위해 csv로도 저장해놓자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set_df.toPandas().to_csv('validation_set_df.csv', index = False)\n",
    "!gsutil cp 'validation_set_df' 'gs://cap-18/output/validation_set_df.csv'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
